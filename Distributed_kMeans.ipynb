{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,neighbors,degree):\n",
    "        self.neighbors = neighbors\n",
    "        self.degree = degree\n",
    "        self.data = None                       #Holds the local data Pi\n",
    "        self.centers = None                    #Holds the centers Bi\n",
    "        self.local_coreset = None              #To store coreset, i.e. Si U Ai\n",
    "        self.weights = None                    #To store the weight of points in local coreset Si U Ai\n",
    "        self.message_received = {}\n",
    "        self.X = None                          #To store the final centers\n",
    "        self.cost_of_each_data = None\n",
    "    def set_data(self,data):\n",
    "        self.data = data\n",
    "    def set_centers(self,centers):\n",
    "        self.centers = centers\n",
    "    def set_cost_of_each_data(self, c):\n",
    "        self.cost_of_each_data = c\n",
    "    def set_local_coreset(self,S):\n",
    "        self.local_coreset = S\n",
    "    def set_weights(self,weights):\n",
    "        self.weights = weights\n",
    "    def set_X(self,X):\n",
    "        self.X = X\n",
    "        \n",
    "communication_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_random_graph(no_of_nodes,probability):\n",
    "    G = nx.erdos_renyi_graph(no_of_nodes,probability)\n",
    "    if nx.is_connected(G):\n",
    "        1\n",
    "    else:\n",
    "        G = create_random_graph(no_of_nodes,probability)\n",
    "    return G\n",
    "\n",
    "def create_preferential_graph(n,m):\n",
    "    # n = number of nodes, m = number of edges\n",
    "    G = nx.generators.random_graphs.barabasi_albert_graph(n,m)\n",
    "    if nx.is_connected(G):\n",
    "        1\n",
    "    else:\n",
    "        G = create_preferential_graph(n,m)\n",
    "    return G\n",
    "\n",
    "def create_grid_graph(n,m):\n",
    "    return nx.grid_2d_graph(n,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get sequence of iterating the nodes\n",
    "#Returns 'nodes' used in fuction arguements later in message passing\n",
    "def node_sequence(G):\n",
    "    seq=[]\n",
    "    l = list(nx.dfs_edges(G,0))\n",
    "    for i in range(len(l)-1):\n",
    "        if (l[i][1] == l[i+1][0]):\n",
    "            seq.append(l[i][0])\n",
    "        else:\n",
    "            seq.append(l[i][0])\n",
    "            seq.append(l[i][1])\n",
    "            p = nx.shortest_path(G,l[i][1],l[i+1][0])\n",
    "            for k in range(1,len(p)-1):\n",
    "                seq.append(p[k])\n",
    "    seq.append(l[-1][0])\n",
    "    seq.append(l[-1][1])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform_partitioning(df,nodes):\n",
    "    temp_df = df.copy(deep=True)\n",
    "    size_of_pi = math.floor(df.shape[0]/len(nodes))\n",
    "    for node in nodes:\n",
    "        if node != nodes[-1]:\n",
    "            node_dict[node].data = temp_df.sample(size_of_pi)\n",
    "            temp_df.drop(node.data.index,inplace = True)\n",
    "        else:\n",
    "            node_dict[node].data = temp_df\n",
    "    return\n",
    "            \n",
    "def similarity_partitioning(df,nodes):\n",
    "    temp_df = df.copy(deep=True)\n",
    "    spec=SpectralClustering(n_clusters=len(nodes), gamma=1.0)\n",
    "    c_id = spec.fit_predict(temp_df)\n",
    "    for i in range(len(nodes)):\n",
    "        node_dict[nodes[i]].data = temp_df[c_id==i]      \n",
    "    return\n",
    "\n",
    "def weighted_partitioning(df,nodes):\n",
    "    temp_df = df.copy(deep=True)\n",
    "    s = np.random.normal(0,1, len(nodes))\n",
    "    s = (abs(s)/ np.sum(abs(s)))*temp_df.shape[0]\n",
    "    for i in range(len(nodes)):\n",
    "        if nodes[i] != nodes[-1]:\n",
    "            node_dict[nodes[i]].data = temp_df.sample(int(round(s[i],0)))\n",
    "            temp_df.drop(node_dict[nodes[i]].data.index,inplace = True)\n",
    "        else:\n",
    "            node_dict[nodes[i]].data = temp_df\n",
    "    return\n",
    "\n",
    "def degree_partitioning(df,nodes):\n",
    "    temp_df = df.copy(deep=True)\n",
    "    s=[]\n",
    "    for node in nodes:\n",
    "        s.append(nx.degree(node))\n",
    "    s = (s/np.sum(s))*temp_df.shape[0]\n",
    "    s = pd.DataFrame(s)\n",
    "    for i in range(len(nodes)):\n",
    "        if nodes[i] != nodes[-1]:\n",
    "            node_dict[nodes[i]].data = temp_df.sample(round(s[i],0))\n",
    "            temp_df.drop(node_dict[nodes[i]].data.index,inplace = True)\n",
    "        else:\n",
    "            node_dict[nodes[i]].data = temp_df\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clustering_algo( data, no_of_centers ):\n",
    "    kmeans = KMeans(n_clusters=no_of_centers, init = 'random', random_state=0).fit(np.array(data))\n",
    "    #init stands for initialization, i.e. how to get first set of centers. Default is k-means++. I went for random\n",
    "    #random_state stands for the seed to generate random centers in the beginning. Kept it zero to get same centers everytime.\n",
    "    #Remove if not required\n",
    "    #The package uses LLoyd's algorithm\n",
    "    return kmeans                        #this will contain all details about clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Message_Passing(message,neighbors,node):\n",
    "    global communication_cost\n",
    "    if node not in node_dict[node].message_received: \n",
    "        node_dict[node].message_received[node] = message\n",
    "    for neighbor in neighbors:\n",
    "        for i, message in node_dict[node].message_received.items():\n",
    "            if i not in node_dict[neighbor].message_received:\n",
    "                node_dict[neighbor].message_received[i] = message\n",
    "                if type(message) == float:\n",
    "                    communication_cost+=1\n",
    "                else:\n",
    "                    communication_cost += message.shape[0]*message.shape[1]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost( data, centers):\n",
    "    distanceMatrix = euclidean_distances( data, centers )\n",
    "    return pd.DataFrame(distanceMatrix).min(axis=1)\n",
    "\n",
    "def distributed_coreset_construction( nodes, t, no_of_centers ):\n",
    "    for node in list(set(nodes)):\n",
    "        print(\"Centers for local node \"+ str(node) +\" being found\")\n",
    "        node_dict[node].centers = pd.DataFrame(clustering_algo(node_dict[node].data, no_of_centers).cluster_centers_)    #this will contain centers stored in dataframe\n",
    "        node_dict[node].centers.columns = node_dict[node].data.columns\n",
    "        print(\"Cost of each data point in node \"+ str(node) +\" being found\")\n",
    "        node_dict[node].cost_of_each_data = get_cost( node_dict[node].data, node_dict[node].centers)  #comes back as pandas series\n",
    "        node_dict[node].cost_of_each_data.index = node_dict[node].data.index\n",
    "    for node in nodes:\n",
    "        print(\"passing cost from node \" + str(node) + \"to its neighbors\")\n",
    "        Message_Passing(node_dict[node].cost_of_each_data.sum(),node_dict[node].neighbors,node)\n",
    "    print(\"\\n\\nReverse message passing for costs begin\\n\")\n",
    "    for node in list(reversed(nodes)):\n",
    "        print(\"passing cost from node \" + str(node) + \"to its neighbors\")\n",
    "        Message_Passing(node_dict[node].cost_of_each_data.sum(),node_dict[node].neighbors,node)\n",
    "    print(\"Message passing of costs done\\n\\n\")\n",
    "    for node in list(set(nodes)):\n",
    "        print(\"calculating ti, m_p, S_i, w_q, w_b for node \" + str(node))\n",
    "        t_i = int(math.floor((t*node_dict[node].message_received[node])/sum(node_dict[node].message_received.values())))\n",
    "        m_p = 2*(node_dict[node].cost_of_each_data + 1e-31)\n",
    "        m_p.index = node_dict[node].data.index\n",
    "        S_i = node_dict[node].data.sample(n=t_i,weights=m_p)\n",
    "        w_q = sum(node_dict[node].message_received.values())/(t*m_p[S_i.index])\n",
    "        w_b = []\n",
    "        for index, b in node_dict[node].centers.iterrows():\n",
    "            print(\"finding points belonging to center \" + str(index) + \"for node \" + str(node))\n",
    "            temp_cost = get_cost(node_dict[node].data,b)   #cost of each data point from b\n",
    "            temp_cost.index = node_dict[node].data.index\n",
    "            Pb = node_dict[node].data[temp_cost == node_dict[node].cost_of_each_data]  \n",
    "            #previous line measures euc dist of each point with center b and compares the values with min cost i.e.min d(p,X)over all x belonging to X\n",
    "            #Pb will be a dataframe\n",
    "            w_b.append(Pb.shape[0] - sum(w_q[S_i.index.intersection(Pb.index)]))\n",
    "        node_dict[node].message_received = {}\n",
    "        node_dict[node].set_local_coreset(pd.concat([S_i,node_dict[node].centers]))\n",
    "        node_dict[node].set_weights(w_q.append(pd.Series(w_b)))\n",
    "        print(\"local coreset for node \"+str(node)+\" created\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distributed_clustering_on_graph(nodes, t, no_of_centers):\n",
    "    print(\"Process begun\\n\")\n",
    "    distributed_coreset_construction( nodes, t, no_of_centers )\n",
    "    print(\"Coresets for all nodes created\\n\\n\")\n",
    "    for v_i in nodes:\n",
    "        print(\"Passing local coreset of node \"+str(v_i)+\" to its neighbors\")\n",
    "        Message_Passing(node_dict[v_i].local_coreset,node_dict[v_i].neighbors,v_i)\n",
    "    print(\"Forward passing of coresets done. \\n Reverse passing begins: \\n\")\n",
    "    for v_i in list(reversed(nodes)):\n",
    "        print(\"Passing coreset of node \" + str(v_i) + \" to its neighbors\")\n",
    "        Message_Passing(node_dict[v_i].local_coreset,node_dict[v_i].neighbors,v_i)\n",
    "    #for v_i in list(set(nodes)):    \n",
    "        #cluster_details = clustering_algo( pd.concat(list(node_dict[node].message_received.values())), no_of_centers )\n",
    "        #node_dict[node].set_X(cluster_details)\n",
    "    print(\"Passing of local coresets completed\\n\\nClustering of global coreset begins\")\n",
    "    cluster_details = clustering_algo( pd.concat(list(node_dict[v_i].message_received.values())), no_of_centers )\n",
    "    return(cluster_details)  #will return details about cluster generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Practice\n",
    "print(\"begin\")\n",
    "G = create_random_graph(10,0.3)\n",
    "G = create_preferential_graph(n,m)\n",
    "G = create_grid_graph(n,m)\n",
    "#Seelect one of the three\n",
    "\n",
    "print(\"Graph created\")\n",
    "nodes = node_sequence(G)\n",
    "node_dict = {i:Node(G.neighbors(i),G.degree(i)) for i in G.nodes()}\n",
    "print(\"Node dictionary created\")\n",
    "\n",
    "df = pd.read_csv(\"letter-recognition.data\", header = None)\n",
    "df = df.loc[:, df.columns != 0]\n",
    "print(\"dataframe read\")\n",
    "\n",
    "uniform_partitioning(df,G.nodes())\n",
    "similarity_partitioning(df,G.nodes())\n",
    "weighted_partitioning(df,G.nodes())\n",
    "degree_partitioning(df,G.nodes())\n",
    "#Select one of the partitioning methods\n",
    "\n",
    "print(\"\\ndata partitioning done\")\n",
    "t =   #please select\n",
    "no_of_centers = 10\n",
    "\n",
    "\n",
    "cluster_details  = distributed_clustering_on_graph(nodes, t, no_of_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practise cell\n",
    "#just testing out anything that i feel like adding to the codes above\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({'a':[1,2,3,4,5,6,7],'b':[3,2,3,4,1,4,5]})\n",
    "df1 = pd.DataFrame({'a':[2,3,4,1,4,6,2],'b':[1,2,4,2,5,6,3]})\n",
    "#df.index.get_loc(df.head())\n",
    "l = pd.DataFrame({'a':[1,2,3,4,5,6,7],'b':[3,2,3,4,1,4,5]}).min(axis=1)\n",
    "k = df.tail(1)\n",
    "df[np.sqrt(np.square(np.array(df) - np.array(k)).sum(1)) == pd.Series(np.array([0,2,3,4,5 , 7, 0]))]\n",
    "test_index=pd.Index(list('23154'))\n",
    "j=[1,2,3,4,5,6,7,8]\n",
    "1-np.array([j[x] for x in list(map(lambda x:test_index.get_loc(x),pd.Index(['1','2','3'])))])\n",
    "k=pd.DataFrame()\n",
    "s={1:df,2:df1}\n",
    "pd.concat(list(s.values()))\n",
    "import networkx as nx\n",
    "j = nx.erdos_renyi_graph(10,0.3)\n",
    "def f(): \n",
    "    global s\n",
    "    s+=1\n",
    "s = 0\n",
    "s = np.random.normal(0,1, 10)\n",
    "\n",
    "s = abs(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
